---
meta:
  title: Setup a Nomad cluster on Scaleway
  description: This tutorial will guide you you through deploying a Nomad cluster with access control lists (ACLs) on Scaleway.
content:
  h1: Setup a Nomad cluster on Scaleway
  paragraph: This tutorial will guide you you through deploying a Nomad cluster with [access control lists (ACLs)](https://developer.hashicorp.com/nomad/tutorials/access-control/access-control) on Scaleway.
tags: compute orchestration nomad cluster consul packer terraform
categories:
  - compute
dates:
  validation: 2023-03-10
  posted: 2023-03-10
---

The tutorial will guide you through the following steps:

0. [Setup](#0-setup) - Setting up the environment
1. [Building the Nomad instance image](#1-building-the-nomad-instance-image) - Building the Nomad instance image

## Prerequisites

- Packer 1.7.7 or later [installed locally](https://developer.hashicorp.com/packer/tutorials/docker-get-started/get-started-install-cli).
- Terraform 1.2.0 or later [installed locally](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli).
- Nomad 1.3.3 or later [installed locally](https://developer.hashicorp.com/nomad/tutorials/get-started/get-started-install).

- A Scaleway account with an [credentials](https://www.scaleway.com/en/docs/identity-and-access-management/iam/how-to/create-api-keys/) setup in your local environment variables (`SCW_ACCESS_KEY` and `SCW_SECRET_KEY` respectively).

## 0. Setup

### Clone the repository

We have created a GitHub repository that should contain all the code you need to deploy a Nomad cluster on Scaleway. Clone the repository to your local machine using the following command:

```shell
git clone https://github.com/scaleway/learn-nomad-cluster-setup.git
```

Go to the `learn-nomad-cluster-setup/scaleway` directory to continue with the tutorial:

```shell
cd learn-nomad-cluster-setup/scaleway
```

### Configuration

In the `scaleway` directory, you will find the following files:

- `image.pkr.hcl` - The Packer configuration file used to build the Nomad instance image.
- `main.tf` - The Terraform configuration file used to deploy the Nomad cluster.
- `variables.tf` - The Terraform variables file used to configure the Nomad cluster.
- `post-setup.sh` - The script used to bootstrap the Nomad cluster.
- `variables.hcl.example` - An example variables file used to configure the Nomad cluster.

We will be using the `variables.hcl.example` file to configure the Nomad cluster. Copy the example file to a new file called `variables.hcl` using the following command:

```shell
cp variables.hcl.example variables.hcl
```

### Configuring Consul tokens

Open the `variables.hcl` file in your favorite text editor and update the following variables:

- `nomad_consul_token_id` - The Consul token ID used to bootstrap the Nomad cluster.
- `nomad_consul_token_secret` - The Consul token secret used to bootstrap the Nomad cluster.

These variables needs to be uuids. You can generate a uuid using uuidgen:

```shell
uuidgen | tr "[:upper:]" "[:lower:]"
```

Or using Terraform console's `uuid()` function:

```shell
terraform console
> uuid()
"4fda5224-6d40-20dd-5dd4-4ce95c1026fb"
> uuid()
"46c99dc9-3536-9a30-8175-92f0b220f688"
```

We will configure the `instance_image` variable in the next step. You can uncomment and configure the rest of the variables as you wish or leave them as they are.

## 1. Building the Nomad instance image

The first step is to build a Nomad instance image using Packer. The image will be built using the [Scaleway Packer builder](https://www.packer.io/docs/builders/scaleway/).

Build the image using the following command:

```shell
packer build -var-file=variables.hcl image.pkr.hcl
```

At the end of the build process, you should see the following output:

```shell
Build 'scaleway.hashistack' finished after 2 minutes 54 seconds.

==> Wait completed after 2 minutes 54 seconds

==> Builds finished. The artifacts of successful builds are:
--> scaleway.hashistack: An image was created: 'hashistack-[random-number]' ...
```

### Update the variables file for Terraform

Use the image name to configure the `instance_image` variable in the `variables.hcl` file.

### Example variables.hcl

At this step your `variables.hcl` file should look like the following **(don't take the same Consul token ID and secret)**:

```hcl
# Packer variables (all are required)
zone = "fr-par-1"

# Terraform variables (all are required)
nomad_consul_token_id     = "4fda5224-6d40-20dd-5dd4-4ce95c1026fb"
nomad_consul_token_secret = "6ae9f6f7-b6f1-4d3a-ea82-bcbea392daa0"
instance_image            = "hashistack-20230310141424"

# The project ID will default to the value of the
# SCW_DEFAULT_PROJECT_ID environment variable or the
# default project ID configured with the Scaleway CLI
# project_id                    = "123e4567-e89b-12d3-a456-426614174000"

# The retry join allows Consul to automatically
# discover other nodes in the cluster. An IAM key will
# be created in Terraform and appended to the retry_join
# variable
# retry_join                    = "provider=scaleway tag_name=consul-auto-join"

# These variables will default to the values shown
# and do not need to be updated unless you want to
# change them
# allowlist_ip                  = "0.0.0.0/0"
# name                          = "nomad"
# server_instance_type          = "PLAY2-NANO"
# server_count                  = "3"
# server_root_block_device_size = 20
# client_instance_type          = "PLAY2-NANO"
# client_count                  = "3"
```

## 2. Deploying the Nomad cluster

Now that we have built the Nomad instance image, we can deploy the Nomad cluster using Terraform.

### Initialize Terraform

Initialize Terraform using the following command:

```shell
terraform init
```

### Deploy the Nomad cluster

Deploy the Nomad cluster using the following command:

```shell
terraform apply -var-file=variables.hcl
```

At the end of the deployment process, you should see the following output:

```shell
Apply complete! Resources: 1 added, 7 changed, 2 destroyed.

Outputs:

IP_Addresses = <<EOT

Client public IPs: 163.172.187.210, 163.172.167.58, 163.172.177.35

Server public IPs: 163.172.146.151, 212.47.247.102, 51.15.229.185

Server public IPs: 2

The Consul UI can be accessed at http://163.172.146.151:8500/ui
with the bootstrap token: 6ae9f6f7-b6f1-4d3a-ea82-bcbea392daa0

EOT
consul_bootstrap_token_secret = "6ae9f6f7-b6f1-4d3a-ea82-bcbea392daa0"
lb_address_consul_nomad = "http://163.172.146.151"
```

### Accessing the Nomad cluster

To setup your local environment to access the Nomad cluster, run the `post-setup.sh` script:

```shell
./post-setup.sh
```

The script will give you the commands to setup `NOMAD_ADDR` and `NOMAD_TOKEN` environment variables:

```shell
The Nomad user token has been saved locally to nomad.token and deleted from the Consul KV store.

Set the following environment variables to access your Nomad cluster with the user token created during setup:

export NOMAD_ADDR=$(terraform output -raw lb_address_consul_nomad):4646
export NOMAD_TOKEN=$(cat nomad.token)


The Nomad UI can be accessed at http://163.172.146.151:4646/ui
with the bootstrap token: 192049fd-52f7-4fa7-7797-b28b78bfcf84
```
